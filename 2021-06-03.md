讨论 2021-06-03
=

## 正确率和误差函数的不一致

神经网络的训练过程离不开误差函数，但在很多时候我们可以在实验中观察到以下情况

- 误差函数最优值和正确率最优值不同时出现，甚至相差很远
- 在测试集上，误差函数和正确率变动程度不一致

这两种现象都表明了正确率和误差函数不一致，我们今天就主要讨论为什么会发生这种情况。

![](https://i.imgur.com/51WuKaH.png )

这里面的首要问题就是交叉熵这些函数和正确率的目标不一样，假如正确的类别已经取得了最高的预测权重，那么这个预测已经是“正确”的，但他们的误差函数可能还很大。

这里实际上会引出正反两部分的考虑

### 决策边界的安全性

![](https://i.imgur.com/EyJxFws.png )
首先我们可以考虑这个过度正确的优势，最重要的用途就是决策边界更安全，比如在新的测试环境里面，由于噪声的干扰，模型的预测分布出现了偏差，那么随之而来的结果就可能被改变，而正确结果和其他结果的区分度决定了这一预测有多容易被干扰。所以一个模型预测的分布如果区分度很高的话，它的抗噪声能力也就更强。


### 过分信任
![](https://i.imgur.com/W0XUp6b.png)

![](https://i.imgur.com/YEMG7TP.png)

另一方面，模型预测过度正确也有很多负面影响，比如过分信任问题，简单来说模型过拟合到最后，对所有判断都是非黑即白，那么可能对一些拿不准的样例也给予极高的预测权重。这就会误导后续使用。


### 应对方法

目前主要的应对方法有两类，一是label smoothing，二是最大熵。
label smoothing就是人为把正确类别的概率设为0.9或一个更小的值，同时把其他错误类别设为一个很小的值，这主要是为了交叉熵考虑的，因为log函数对这些边界取值比较敏感。二是最大熵原理，即在保证训练集正确的条件下，尽可能让预测的熵变大。

\begin{gather}
\mathrm{argmin}_\theta  \quad -\sum_{x,y^* \in D} \log P(y^*|x) - H(P(y|x;\theta)) \\
\end{gather}
