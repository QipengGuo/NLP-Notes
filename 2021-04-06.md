讨论 2021-04-06
=

## 前言

欢迎各位同学的参与，本学期的分享将围绕几个小专题展开，每个专题对应一种方法论，比如Cycle-consistency、Contrastive Learning等。每个专题除了介绍基础概念和一些已有工作外，也将给出一些个人见解和一些可做的idea，欢迎感兴趣的同学尝试。

# Cycle-consistency

为方便起见，我们将Cycle-consistency译为环形一致性，该方法的基本思想是跨模态的自编码器，即把原始输入经过一个环之后再次复原。假如有两个数据分布$X$和$Y$，环形一致性即指$X \rightarrow Y \rightarrow X$和$Y \rightarrow X \rightarrow Y$，这里面的$X,Y$指两个没有交集或交集足够小的数据分布，比如文本和图像，知识图谱和文本，中文和英文等。

## 基本形式

\begin{gather}
f : X \rightarrow Y, \quad g: Y \rightarrow X, \\
L_{X2X} = \mathbb{E}_{x \sim X} D(g(f(x)), x), \\
L_{Y2Y} = \mathbb{E}_{y \sim Y} D(f(g(y)), y), \\
\end{gather}
 
其中$D$为度量函数，可为交叉熵，欧式距离等。

需要注意的是环形一致性通常需要两个环同时存在，在没有附加条件下，只取一个是有问题的。

可以看出如果把两个数据分布中的一个换为隐变量并只取一个环，那么环形一致性方法就退化为了自编码器。

## 理论问题
* 环形一致性有无穷解，无法保证优化目标有意义
* 如果保证$f,g$是对应空间的映射函数
* 必须存在一组互为可逆函数的$f,g$，优化过程才可能收敛到有价值的固定点，即两个空间存在一一对应关系
* 环形一致性的可解性依赖于数据分布存在低维流形的假设

### 无穷解问题
假设$X$的样本点有A,B,C，$Y$的样本点有1，2，3，那么存在多种$f,g$都可以满足环形一致性。当两个空间连续时，就存在无穷多种$f,g$满足环形一致性。如何保证二者是有意义的便成为了一个重要的研究问题，目前主流方法可分为融合先验知识法和辅助任务法。前者指设计特定的模型结构，让其初值或解空间具备某种物理意义。后者则利用预训练或辅助任务，先训练模型参数，让其参数有一定意义。

这里简单介绍下辅助任务的方法，因为融合先验知识的方法需要因地制宜，结合不同任务有不同处理方式。

[Denoising Autoencoder](https://openreview.net/pdf?id=rkYTTf-AZ)
噪声自编码器即在输入信号中加入噪声干扰，再让模型复原完整信号。在环形学习的过程中，模型需要接受另一模型生成的伪造样本，该方法认为伪造样本和带噪声样本之间存在一定关联性，让模型熟悉带噪声样本有助于处理伪造样本。同时，噪声自编码器也可以让模型具备良好的初始状态。当然，这种方法其实只适用于两种数据分布是同构的情况，比如中文和英文，如果是图像和文字，这种方法会有一定的问题。

![](https://i.imgur.com/80Xc04S.jpg)


[MASS](https://arxiv.org/pdf/1905.02450.pdf)
专为无监督机器翻译设计的预训练任务，利用mask来模拟翻译的过程，即参照原文生成新的文字。
![](https://i.imgur.com/hBGGcph.png)

[adversarial training](https://openreview.net/pdf?id=rkYTTf-AZ)
![](https://i.imgur.com/VRuqIbf.png)
### 输出空间合法性
环形一致性要求$f,g$两个函数可以把数据点从一个空间映射到另一个空间，但实际使用中，由于二者都是带参数的模型，很难保证它们是合法映射，即$f(x)$不一定在$Y$空间中，反之$g(y)$也不一定在$X$空间中。如果发生这种情况，那么环形一致性就变得无意义，因为可以有$f(x)=x,g(y)=y$这样的解存在。

同样的，我们可以引入先验知识，从模型结构角度限制输出空间。不过这类方法同样需要结合具体任务进行分析。

另一种做法是引入[adversarial training](https://arxiv.org/pdf/1703.10593.pdf)，训练判别器来考察两个函数的输出点是否属于要求的空间。
![](https://i.imgur.com/YYlwSnn.png)



其实还有一种比较巧妙的方法来解决前两个问题，即通过self-training或其他方法构建弱监督信号，以此训练模型。弱监督的缺点是监督信号不够准确，但足以解决以上两个问题，而环形一致性又可以提供一个严格的优化目标，综合二者一般可以得到很好的效果。

### 双射假设
假如两个空间不存在一一对应关系，设想有$x_1 \rightarrow y_1,x_2 \rightarrow y_1$，那么$D(g(f(x_1)), x_1),D(g(f(x_2)), x_2)$互相矛盾，二者始终无法同时降为0，并且模型会收敛到$x_1,x_2$的某种中间点（视度量函数而定），这很有可能是错误的点，甚至不在$X$空间内。如果这类非一对一关系过多，会严重干扰到模型的正常训练过程。

为此，我们可以利用引入隐变量的方式来消除歧义，假定是一对多的情况，可以有
\begin{gather}
f : X \rightarrow Y, \quad g: Y \rightarrow X, \\
L_{X2X} = \mathbb{E}_{x \sim X} D(g(f(x), z), x), z \sim P(z|x) \\
L_{Y2Y} = \mathbb{E}_{y \sim Y} D(f(g(y, z)), y), z \sim N(0,1) \\
\end{gather}
![](https://i.imgur.com/QncLjok.png)

![](https://i.imgur.com/V95MkIb.png)
![](https://i.imgur.com/UDYkL4u.png)


### 低维流形假设

假设两个空间是文本和经过加密的文本，那么环形一致性就转变为了组合优化问题，求解复杂度极高。换言之，现行的环形学习都假设了两个数据分布都服从低维流形的控制，所以模型可以在可接受时间内找到双向对应关系。

# 下次预告

## 相关工作

## 一些简单的idea
