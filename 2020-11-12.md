讨论 2020-11-12
=

# 经典模型及算法导览 （二）
## Generative Adversarial Network (GAN)
首先，GAN模型也是个生成模型，其目的是通过上次讲到的逆变换采样，把已知的噪声样本映射到数据样本，但不同于VAE，GAN通过一个博弈游戏来找到最合适的映射。简单来说，GAN设定了两个角色，一个是造假者，一个是审查者，造假者需要尽可能地骗过审查者，而审查者需要尽可能分辨真伪。而造假者就是我们最终要的生成器。具体形式如下

$$ \mathrm{argmin}\_\theta \mathrm{argmax}\_\phi \mathbb{E}\_{z\sim P_{data}} \log D_\phi(z) + \mathbb{E}\_{z\sim P_\theta} \log (1-D_\phi(z)), \quad P_\theta(z) = P(z=G(z';\theta)),z'\sim N(0,1) $$
其中G，D是两个模型，对应造假者和审查者。简单分析上式，不难发现D的最优取值是$\frac{P_{data}}{P_{data}+P_\theta}$。当D是唯一变量时，对原式求导即可得到。
\begin{align}
& \mathbb{E}\_{z\sim P_{data}} \log D(z) + \mathbb{E}\_{z\sim P_\theta} \log (1-D(z)) \\
= & \sum_z P_{data}(z) \log D(z) + P_\theta(z) \log (1-D(z)) \\
\nabla_{D(z)} = &  P_{data}(z) \frac{1}{D(z)} + P_\theta(z)\frac{1}{1-D(z)} * (-1) = 0 \\
& P_{data}(z) \frac{1}{D(z)} = P_\theta(z) \frac{1}{1-D(z)} \\
& D(z) = \frac{P_{data}(z)}{P_{data}(z)+P_\theta(z)} 
\end{align}

此时，再将D的最优值代回原式，可得
$$ \mathbb{E}\_{z\sim P_{data}} \log \frac{P_{data}}{P_{data}+P_\theta} + \mathbb{E}\_{z\sim P_\theta} \log (1-\frac{P_{data}}{P_{data}+P_\theta}) $$
显然，$P_{data} = P_\theta$时，上式最大，为$-\log2-\log2 = -\log4$，我们之后在把原式减去$-\log4$得
\begin{gather}
\mathbb{E}\_{z\sim P_{data}} \log \frac{P_{data}}{P_{data}+P_\theta} + \mathbb{E}\_{z\sim P_\theta} \log (1-\frac{P_{data}}{P_{data}+P_\theta}) -(-\log4) \\
= \mathbb{E}\_{z\sim P_{data}} \log \frac{2P_{data}}{P_{data}+P_\theta} + \mathbb{E}\_{z\sim P_\theta} \log (\frac{2P_\theta}{P_{data}+P_\theta}) \\
= \mathrm{KL}(P_{data}|\frac{P_{data}+P_\theta}{2})+\mathrm{KL}(P_\theta|\frac{P_{data}+P_\theta}{2}) \\
= 2\mathrm{JSD}(P_{data}||P_\theta)
\end{gather}
所以最优的D其实是$2\mathrm{JSD}(P_{data}||P_\theta)-\log4$
由此可以看出GAN为什么有效，它本质上拉近了数据分布和生成分布之间的JSD距离，这是非常合理的，也是生成模型的一大追求。但需要注意的是只有在D取最优值时，它才度量了JSD距离，如果不是最优，没有任何保证。所以GAN的收敛性也是假定每步D先收敛到最优，G再收敛，如此迭代直到二者平衡。在实际中，我们也要保证D的学习略快于G，才能提供良好的学习信号。

GAN的基本知识我们讲完了，下面讲GAN在文本生成中的用途。
## Auto regressive model
自回归模型是序列生成的标配，但不是唯一选择，近年来，非自回归的方法层出不穷，但两种思路各有利弊。自回归本质是链式分解，即$P(x_0,x_1,\cdots,x_{n-1})= x_0 \prod_i P(x_i|x_{<i})$，需要注意，这里$x_0,x_1$的顺序是任意的，一句话从左到右，从右到左，跳着数都可以。所以自回归的意思就是把估计原本的联合概率，变成依次估计多个小的条件概率。这方法固然把一个复杂问题拆分为了很多个简单问题，但也带来了麻烦。如果把模型的预测定为$\hat{x}_0,\hat{x}_1,\cdots$，那么在生成时，实际的概率为$P(x_i|\hat{x}_{<i})$，虽然我们的目标就是让$x_i$和$\hat{x}_i$尽量接近，但除非达到最优值，二者都是不相等的，那么我们生成时所用概率与原始分解不同，结果不能保证正确。这也就是经典的 **Exposure bias** 问题。 实际上链式分解所带来的这个问题几乎是不可根本性消除的，它始终需要已生成的序列足够接近数据分布，但在序列生成中误差累计不可避免。大部分研究此问题的工作采用一种这种方案，不是解决问题而是改变题干，不要求严格符合数据分布，而是符合数据和生成结果的混合分布。这样就减轻了Exposure bias的影响，但代价是我们不再是严格追求对数据分布的建模了。

## Policy Gradient
如果一个序列整体有得分，如何将整体得分变为每步的得分？这就需要RL出场了。
假如目标为优化$\mathbb{E}\_{\tau\sim P_\theta(\tau)} r(\tau)$，其中$\tau$为决策路径，$r$为此方案的得分。
\begin{align}
\nabla_{\theta} \mathbb{E}\_{\tau\sim P_\theta(\tau)} r(\tau) =& \sum_\tau \nabla_\theta P_\theta(\tau) r(\tau) \\
=& \sum_\tau P_\theta(\tau) \nabla_\theta \log P_\theta(\tau) r(\tau)
\end{align}
这样我们就把求导过程从期望外面拿进了期望里面，可以对每个路径进行优化了。
又因为我们采用自回归模型$P_\theta(\tau) = x_0 \prod_i P_\theta(x_i|x_{<i})$所以最终有$\mathbb{E}\_{\tau \sim P_\theta(\tau)} r(\tau) \nabla_\theta  \sum_i P_\theta(x_i|x_{<i})$，至此我们就把整句话的得分拆分到了每一步预测中。
## Text GAN

![](https://i.imgur.com/Hwwncas.png)


## 预告
### GAN和VAE的更多拓展及实例

