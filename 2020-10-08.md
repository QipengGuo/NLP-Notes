讨论 2020-10-08
=

# NLP中的常用模型 （一）

## RNN 循环神经网
假设有一个序列$X=x_0, x_1, x_2, ..., x_n$，与HMM类似，RNN通过递推的方式来建模这个序列，$$ h_t = \mathrm{tanh}(Wx_t+U h_{t-1}) $$

这种方式的好处是同一个模型可以应对任意长度的序列。同时RNN也有很多性质，而其中不乏双刃剑，比如RNN建模序列式可以考虑相对次序，输出与$t$无关。这在很多场景下是优势，比如类周期性的信号，没有明确的开始与结束。但同时有时确实劣势，因为与$t$无关也就意味着RNN不能感知绝对位置。但假如了解RNN的特性，这一缺点完全可以规避。\<BOS\> \<EOS\>就可以解决这一问题。

在对RNN的研究中，最具代表性的就是长时记忆问题，也就是探讨RNN记忆力如何的问题。实际上，在应用RNN的过程中，许多人都发现了RNN不能记住多步之前的信息（通常指几十步以上）。对此主流的解释是梯度弥散和梯度爆炸问题

\begin{align}
\frac{\partial h_t}{\partial h_{t-1}} &= (1-h_t^2)U \\
\frac{\partial h_t}{\partial h_{t-k}} &= \prod_{i=t-k+1}^t (1-h_i^2) U^k
\end{align}

我们可以很容易发现，k步间的导数传递十分不问题，很容易快速增大或快速减小。也就带来了梯度爆炸和梯度弥散问题。另外这种现象同样存在于深层网络中，因为相应的项从$U^k$变为$\prod_i U_i$同样很不稳定。同时根据这一推导我们也可以理解为甚RNN难以记住几十步以前的信息。并且我们发现只有$U$接近于1，RNN才可能稳定这一思想也影响了后续众多对RNN的改进工作。

### 正交初始化
假如矩阵$A$满足$A^TA=AA^T=I$，则A为正交阵，同时正交阵有一个重要性质，即模长不变
\begin{align}
||x||_2 &= x^T x \\
||Ax||_2 & = (Ax)^T Ax \\
         & = x^T A^T A x \\
         & = x^T x \\
         & = ||x||_2 \\
\end{align}
知道了经过正交矩阵不会改变模长，我们就想到如果 $U$是一个正交矩阵，那么长距离梯度传递问题是不是就得到很好解决？
受这种想法启发，现在很多RNN的实现都会把 $U$初始化为一个正交阵。但遗憾的是在优化过程中很难保持正交性质，把$U$重参数化，让其强制为正交阵又会使网络能力大大下降。

### 激活函数的选择
我们已经知道了tanh的导数，并且发现它在0~1之间的，这不利于长距离梯度传递。所以也有一些工作探讨了是否有其他更适合RNN的激活函数，比如使用ReLU替代tanh，我们知道ReLU在正数部分的导数就是1，所以很有利于长距离传递。一些研究也表明这种改进在一些情况下确实提高了RNN的记忆能力，但我的个人经验是这种结构非常脆弱，很难调。（大家可以思考一下为什么比tanh难调）

### 集大成者 LSTM
LSTM既考虑了长距离梯度传递问题，又兼顾了网络的表达能力，看似复杂，其实考虑周全。
\begin{align}
i_t &= \sigma(Wx_t+Uh_{t-1}) \\
o_t &= \sigma(Wx_t+Uh_{t-1}) \\
f_t &= \sigma(Wx_t+Uh_{t-1}) \\
\hat{h}\_t &= \mathrm{tanh}(Wx_t+Uh_{t-1}) \\
c_t &= f_t * c_{t-1} + i_t * \hat{h}\_t \\
h_t &= o_t * \mathrm{tanh} (c_t)
\end{align}
注意每一行的$W$和$U$都是不同参数。 我们可以发现$c_t$是一条记忆的通路，与$U$无关，只收gate的控制。假如gate常开，LSTM可以允许长距离的梯度传输。 但也要注意，$f_t$ 是sigmoid函数出来的，所以初值不会太接近1，而指数衰减速度很快，所以想要学习长距离关系，一般要把$f_t$相应的参数单独初始化，让其初始值就接近1。

## 预告
### 外部记忆
### NTM 神经图灵机
### RNN中的Attention
### 层次化的RNN
### 超越序列的RNN
