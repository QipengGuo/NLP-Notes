讨论 2019-10-21
=

# Seq2Seq
- ## seq2seq 模型是如何理解翻译问题的
- ## seq2seq 模型是如何解决翻译问题的 
- ## seq2seq 模型的视角有何特点（略，seq与struct的联系的区别，text seq与action seq的关系）

## “按需说话”
seq2seq模型把翻译问题考虑为一个按照一定需求或条件进行文本生成的问题，同时还限定了输入的条件和输出的目标都是以序列为载体的。以序列为载体阐明了seq2seq模型对文本结构的态度，尽管有很多工作将文本中的结构信息引入seq2seq模型，但最基础的seq2seq模型只考虑了文本中最基本的结构信息，即顺序信息，或者说把文本当作一个数据链（linear-chain）。

## encoder-decoder模型
早期的seq2seq模型其实是一个两段式的模型，是由seq2vector和vector2seq两部分组成的，这两部分功能分别由encoder和decoder两个模块完成。整个流程如下图所示
![](https://i.imgur.com/Yb5Iv2k.gif)

### RNN-based

$\text{src}=\text{Embedding}(w_1,w_2,w_3,\cdots,w_N)$
$H^e = \text{LSTM}(\text{src})$
$z = \frac{1}{N} \sum_i h^e_i$
$h^d_0=\mathbf{0}$
$h^d_t = \text{LSTM}([\text{tgt}_t;z],h^d_{t-1})$
$y_t=\text{softmax}(\text{MLP}(h^d_t)$)



# Seq2Seq with Attention
- ## 为什么要在seq2seq模型中引入attention
- ## attention解决了什么问题，引发了什么新问题

## seq2seq遇到的问题
seq2seq模型在提出后不久就暴露出了很多实验上的问题，比如decoder的表现往往是虎头蛇尾（开始很好，后续词的生成质量越来越差），翻译出的句子很多时候过于偏向高频词的堆叠，流畅但不达意。基于这些观察到的问题，一部分研究认为只用一个向量来表示整个源语言的输入杯水车薪，应该让模型更好的访问源语言句子的细节而不是一个高度概括的向量。

## Attention的物理含义
Attention机制在机器翻译领域取得了很大的成功，所以也有大量的研究者对Attention机制的原理提出了解释。在此只罗列几种简单的，一是动态选择说，这种假说对Attention过程提出了两种观点，把Attention Score类比为相似度，把Softmax函数类比为argmax的选择。第二种假说是二元关系说，主要观点与Relation Networks一致，认为Attention建模了pair-wise relations。


![](https://i.imgur.com/uymmAW1.gif)

### RNN-based

$\text{src}=\text{Embedding}(w_1,w_2,w_3,\cdots,w_N)$
$H^e = \text{LSTM}(\text{src})$
$h^d_0=\mathbf{0}$
$z_t = \text{LSTM}([\text{tgt}_t;h^d_{t-1}], z_{t-1})$
$h^d_t = \text{Attention}(z_t, H^e)$
$y_t=\text{softmax}(\text{MLP}(h^d_t)$)

### Transformer

$\text{src}=\text{Embedding}(w_1,w_2,w_3,\cdots,w_N)$
$\text{pe} = \text{Embedding}(1,2,3,\cdots,N)$
$H^e = \text{SubLayer}(\text{src}+\text{pe},\text{src}+\text{pe})$
$\text{SubLayer}(x,y)=\text{LayerNorm}(z+\text{FFN}(z))$
$z = \text{LayerNorm}(x+\text{Attention}(x,y))$
$h^d_t = \text{SubLayer}(\text{SubLayer}(\text{tgt}_t,h^d_{<t}), H^e)$

# 后续安排
- 大家想听哪些方面的内容，模型，理论，任务场景，实验技巧
- 互动形式，有奖问答？（例如赠书）
- 对我的建议
