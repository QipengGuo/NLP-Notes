讨论 2020-10-29
=

# 经典模型及算法导览 （一）
## Variational Auto Encoder（VAE）

VAE是一个我个人很喜欢的生成模型，我认为在很多方面它都是优于GAN和Normalizing flow的，但本次我们不深入这个话题。以VAE本身的推导和介绍为主。

首先，与所有生成模型一样，VAE试图找到一种方式来建模$P(X)$，即数据分布，更具体来讲，我们希望找到一种方式，可以不停的从$P(X)$采样出数据点。而VAE是通过分布变换来实现这样一点的，分布变换指的是从一个分布中采样得到一个样本$z$，通过一个确定性函数$f(\cdot)$可以把$z$映射到另一分布。这种采样方法也称之为逆变换采样。

### 逆变换采样
对一个随机变量X，它的累计分布函数,
$$F(x)=P(X\leq x)=\int_{-\inf}^x p(x) dx $$
假如存在严格单调递增函数$T: [0,1] \rightarrow R$，我们可以把0，1间的均匀分布$U$映射到实数轴上，让$T(U)=X$，所以，
$$F(x)=P(X\leq x)=P(T(U)\leq x)=P(U\leq T^{-1}(x))$$
又因为$U$的累计分布在0,1区间就是$F(u)=u$，所以上式变为$F(x)=T^{-1}(x)$,可推出$F^{-1}(u) = T(u)$，即$F^{-1}(u)=x$
所以我们可以通过从均匀分布采样来得到任意分布的采样点。

VAE的目标就是通过auto encoder的形式，找到这个映射函数。

### VAE推导方法1

\begin{align}
\log P(x) =& \log \sum_z P(x,z) \\ 
=& \log \sum_z P(x,z) \frac{Q(z|x)}{Q(z|x)} \\
=& \log \sum_z Q(z|x) \frac{P(x|z) P(z)}{Q(z|x)} \\
=& \log \mathbb{E}\_{z \sim Q(z|x)} \frac{P(x|z) P(z)}{Q(z|x)} \\
\geq& \mathbb{E}\_{z \sim Q(z|x)} \log \frac{P(x|z) P(z)}{Q(z|x)} \\
\geq& \mathbb{E}\_{z \sim Q(z|x)} [\log P(x|z) + \log P(z) -\log Q(z|x)] \\
\geq& \mathbb{E}\_{z \sim Q(z|x)} \log P(x|z) - \mathrm{KL}(Q(z|x)||P(z))
\end{align}

我们可以发现Q(z|x)可以被替换成任意分布，但它的影响是什么呢？

### VAE推导方法2
\begin{align}
\mathrm{KL}(Q(z|x)||P(z|x)) =& \mathbb{E}\_{z \sim Q(z|x)} [\log Q(z|x) - \log P(z|x)] \\
=& \mathbb{E}\_{z \sim Q(z|x)} [\log Q(z|x) - \log P(x|z) - \log P(z) + \log P(x)] \\
=& -\mathbb{E}\_{z \sim Q(z|x)} \log P(x|z) + \mathrm{KL}(Q(z|x)||P(z)) + \log P(x) \\
\log P(x) =& \mathrm{KL}(Q(z|x)||P(z|x)) + \mathbb{E}\_{z \sim Q(z|x)} \log P(x|z) - \mathrm{KL}(Q(z|x)||P(z)) \\
\log P(x) \geq& \mathbb{E}\_{z \sim Q(z|x)} \log P(x|z) - \mathrm{KL}(Q(z|x)||P(z))
\end{align}

由第二种方法可知，$\mathrm{KL}(Q(z|x)||P(z|x))$决定了VAE所求的ELBO和真实$P(x)$之间的差距，所以$Q$的选取是非常重要的。

### 先验分布
很多工作假设$P(z)$为标准正态分布，这个假设可以这样理解，
$$P(z) \sim N(0,1), P(z)=\sum_x P(z,x)=\sum_x P(z|x)P(x)=\mathbb{E}\_x P(z|x) $$
也就是说我们需要数据集所对应的z满足标准正态分布，而不是每一个样本点。
另外P(z)可以任意选择，甚至是带参数，可学习的。

### mini-batch
上述推导中没有涉及数据集的问题，实际上，我们要求的不是 $\max P(x)$而是$\max \frac{1}{N} \sum_x P(x)$.
所以相应的VAE目标为
\begin{align}
\max \quad & \frac{1}{N}\sum_x [\mathbb{E}\_{z \sim Q(z|x)} \log P(x|z) - \mathrm{KL}(Q(z|x)||P(z))] \\
\max \quad &  \frac{1}{N}\sum_x \mathbb{E}\_{z \sim Q(z|x)}   \log P(x|z) - \frac{1}{N}\sum_x \mathrm{KL}(Q(z|x)||P(z))] 
\end{align}

我们可以看到，当使用SGD方法时，对前后两项的估计可能误差是不一样的，所以当batch size很小时，会增加一个对KL项的额外系数，让其在整个epoch上和为1.

### VAE解释1
VAE的第一种解释是噪声信道，Encoder $Q(z|x)$的输出会经过一个噪声信道，传递给 Decoder $P(x|z)$，而KL项决定了噪声的强弱。

### VAE解释2
VAE追求最小表示，不论z的维度设定为多少，KL项会使model用尽量少的表达能力去传递信号。这等同于用最小的维度来传递信息，因为可以找到一种映射方法，让某一些维度尽量稳定来高效的传递信息，而让其他维度贴近噪声分布。

### 两种collapse
一是KL项太大，model完全不使用z，二是KL项太小，decoder退化
 
### CVAE
$$\log P(x|c) \geq \mathbb{E}\_{z \sim Q(z|x)} \log P(x|c,z) - \mathrm{KL}(Q(z|x)||P(z|c))$$
事实上，$P(z|c)$可以对于每个$c$都是标准正态分布，实际就退化为了$P(z)$
