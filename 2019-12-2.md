讨论 2019-12-2
=

# 机器翻译在文本生成任务的定位

NLP中有众多的文本生成任务，本次讨论会涉及有代表性的几种任务，分别是NLG（狭义，特指data-to-text），摘要，翻译，对话。讨论的重点会在于机器翻译与其他任务之间的联系和区别。

## NLG 简介
介于其他任务大家了解较多，就不在赘述，这里只简单概述下传统NLG的任务场景。

https://nlgsummer.github.io/slides/Ehud_Reiter-Intro_to_NLG.pdf

例子：天气预报
给定一系列天气相关的数据，产生一段文字播报。

例子：路况提示
给定一系列道路信息（例如堵车，大雪，修路，事故），产生文字播报。
需要考虑轻重缓急，消息有优先级，每次生成内容或多或少，不固定。

**Overview** Road surface temperatures will reach marginal levels on most routes from this evening until tomorrow morning.
 **Wind (mph)** NW 10-20 gusts 30-35 for a time during the afternoon and evening in some southwestern places, veering NNW then backing NW and easing 5-10 tomorrow morning.
**Weather** Light rain will affect all routes this afternoon, clearing by 17:00. Fog will affect some central and southern routes after midnight until early morning and light rain will return to all routes. Road surface temperatures will fall slowly during this afternoon until tonight, reaching marginal levels in some places above 200M by 17:00. 

## 信息角度
从文本生成过程中的信息传递角度出发，
- NLG：信息或增或减，取决于原数据的抽象程度，如果类似AMR则信息基本恒定，但如果信息过多，例如路况信息，输出文本只包含了重要部分，而对于天气预报类，要补充语言知识和常识。
- 摘要：信息减少，10句话变1句话，显然浓缩
- 翻译：不多不少，信达雅，第一条“信”便要求信息不增不减
- 对话：信息不是核心，同感和共情更为重要，任务相关的对话也强调信息传递，但通常可以看做应用问答或其他技术，对话系统本身重点不在此。

但从文本生成角度出发，信息应不增不减，对信息的调整更多设计语言理解和语义关系，常识等。以此出发，机器翻译具有优势，但同时也有劣势，从信息角度可以看出，机器翻译不一定需要语言理解，或者说在有些情况下不需要语言理解。

## 复杂性角度
研究问题要讲控制变量，从任务场景复杂性来讲 NLG < 摘要 < 翻译 < 对话，但从任务本身来讲 翻译 < 摘要 < 对话 < NLG

这部分比较主观，仅供参考
### 任务场景
NLG简单来讲就是 结构化数据 到 文本的过程，输入格式往往是固定的，比如天气预报，就总是那个输入数据，内部逻辑高度一致，几乎所有数据都是一个模型。 
摘要就是挑些重点，再组织成一句话，并且大部分输入也是想尽的领域，比如都是新闻。
翻译相对就复杂一些，第一是输入高度自由，第二是模式不固定，陈述句，主动，被动，问句，感叹句，都要翻译，难以一言蔽之。
对话最为复杂，要考虑的因素也最多。

### 任务本身
翻译在相似语言中就是复述，遣词造句都可以模仿输入，不需要深入理解语义。不相似语言之间有些难度，但天下语言无非几大类，共通点很多。
摘要，摘抄些句子在合并即可，比翻译复杂在挑选句子和合并。合并本身并非易事。
对话，考虑很多，但可以通过状态机模拟很多场景。
NLG，很多输入是不包含任何语言信息，例如一组纯数字，无中生有最难，所以NLG很难有通用结构。

## 评测角度
从两点出发，一是答案的数量和多样性，二是度量指标的合理性。
首先从答案的分布出发，翻译问题的歧义性最少，换言之最多只有几种正确的翻译的方法，而其他几种都高度多样化。试想对话场景中，会有非常多不同说法。
再者，度量指标（BLEU，ROUGE）大部分受长度影响很大，所以想要准确的度量，首先要保证长度与参考标注一致，而这一点其实只有翻译可以做到。
所以从评测角度来说，翻译是目前最标准的，但也差很多。
