讨论 2021-06-17
=

## 浅谈Neural ODE

## 固定时间步，点过程，和连续时间
对于时间序列，有三种理解方式，一是rnn常用的时间序列，它通常是等时间间隔的离散序列，也就是序列中每个元素间相隔时间相等。二是点过程，即元素间时间间隔不想等，比如第一分钟有个信号，一小时后第二个信号，五分钟后第三个信号。三是连续时间，这时就不能用序列表示了，信号是一个连续的信号，比如声音，无线电。

今天要讲的就是关于连续时间过程，首先，我们应该考虑连续时间和离散时间序列间的联系。实际上，如果把目标随时间变化的函数定义出来，同时这个变化就对应了目标函数对时间的导数。我们把这个函数记为$f()$。那么离散时间序列就可以看作对连续时间过程的分段积分。对任何一段时间间隔，已知其起点状态，我们可以知道终点状态应该为

\begin{gather}
\frac{dz}{dt} = f(z(t);\theta) \\
z(t_1) = z(t_0) + \int_{t_0}^{t_1} f(z(t);\theta) dt
\end{gather}


## Neural ODE

观察这个形式可以发现，新的时间点等于旧的时间点加上积分路程的变化，这和残差网络在形式上是相近的。所以我们同样可以考虑用神经网络代替变化函数。

假如时间间隔是固定的，这个积分过程就是一个rnn。但如rnn不同的是我们可以定义变化的间隔，以及任意精度的时间间隔，比如0.5时间步，3.1415926时间步等。我们可以认为这是一个把时间间隔作为参数的，可以建模点过程的模型。

当然，有一个显著问题是既然没有固定时间步的概念，序列的长度就很自由，并且小问题也可以看作长序列，所以学者们还讨论了一些梯度估计的方法，用于超长序列的回传。这里只介绍最简单的adjoint method。

## Adjoint Method

梯度回传其实是一个逆时间方向，进行回溯的过程，所以他同样是可以用常微分当成来建模，当然，我们既然知道正向时间是神经网络，所以反向也是有界的，是可积的。所以adjoint method把反向过程在时间步之间的部分再次用常微分当成求解，对一段区间的导数变化进行积分。

这样的好处是你可以调整反向传播的精度和效率，结合经典算法和正向得到的信息，可以自适应的调整反向过程。

接下来就是讲解如何求反向过程的变化函数，实际上他是有解析解的，可以通过网络结构求出，下面进行推导。

（这个推导非常好，就直接粘贴了）
https://personal.ntu.edu.sg/lixiucheng/paper/neural-ode.html 

到z(t)之后还差一步，就是求出对参数的导数。这里学者提出了一种比较不直观的方法，把参数也看做时间的函数，只不过它对时间的导数为0。同样的，如果有需要，也可以把时间看作时间的函数，导数就是1。

这样，把参数和时间扩充到状态中，即可求出相应的导数，再进行更新。

今天的介绍只是粗浅的介绍了 neural ode相关的内容，实际有很多技巧和高端用法没有提及，大家感兴趣可以自己查阅资料。
