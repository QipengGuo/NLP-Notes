讨论2021-04-20
=

# Pre-training 第一讲

## 为什么需要预训练
预训练模型在各个深度学习领域中均大放异彩，取得了诸多成果，是近年来热度最高的方向之一。尽管预训练方法在大量任务上取得了显著的效果，但一个根本性的问题是为什么需要预训练模型，预训练过程对最终模型究竟产生了何种影响？




### 预训练作为正则项
首先，一种非常经典的观点是预训练可以视作一种正则约束，即模型参数在一定区域内不受惩罚，在其他区域受到惩罚，从而把模型参数约束在一个特定范围内。不论预训练的任务如何选取，其本质仍是一个优化问题，假如预训练和最终任务的定义域相同或者有很大交集，预训练的优化过程就把模型参数限定在了一个能取得很好的预训练误差的区域内。鉴于预训练任务通常是数据量很大，误差函数简单，可以认为其误差平面比较平滑，在这种条件下，假如后续训练信号强度不高时，模型很大概率在训练后仍在预训练误差较小的区域内。

### 半监督学习

另一种观点是把预训练和半监督学习联系到一起，比较经典的形式就是考虑
$$P(X,Y|\theta)+P(X|\theta)$$
即找寻同时满足联合概率和边际概率的参数$\theta$，在很多机器学习模型中，对$P(X|\theta)$的估计是有利于估计$P（Y|X;\theta）$或$P(X,Y|\theta)$的。比如朴素贝叶斯等。另一方面，对边际概率的估计至少不能干扰模型的学习（理想情况下），因为假如存在一个参数可以良好地建模联合概率，那么它同样可以描述边际概率。

当然，很多实验都证明了预训练或无监督对模型学习可能是有害的，这是因为我们设定的的模型空间中并不包含正确解。即最优化的$\theta^*$并不在我们的解空间中，在这种情况下，会差生两个误差项$e_l,e_u$前者代表有label时监督学习的误差，后者代表无监督或预训练的误差。当二者数据比例改变时，这两个误差项的占比也随之改变。所以会出现引入更多的无监督和预训练反而使模型变差的问题。

### 知识/关系迁移
除了上述的理论外，还有一种不同的观点，考虑以下例子，RoBERTa的训练集由以下几个部分组成
- BOOKCORPUS plus English WIKIPEDIA. This is the original data used to train BERT. (16GB).
- CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset. The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).
- OPENWEBTEXT, an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).
- STORIES, a dataset containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB).

BookCorpus
|\# of books| # of sentences| # of words| # of unique words mean| # of words per sentence median| # of words per sentence|
|-|-|-|-|-|-|
|11,038|74,004,228| 984,846,357| 1,316,420| 13| 11|

For Winograd schemas, an example:
The trophy doesn’t fit in the suitcase because it is too big. What is too big? 
Answer 0: the trophy. Answer 1: the suitcase

除了OPENWEBTEXT是从Reddit上抓取的之外，其他来源都比较清晰，并且OPENWEBTEXT也说明其滤掉了非英文页面，并作了tokenize。所以RoBERTa很大程度上是不了解代码，程序语言的。而下图中展示了其在代码搜索中的效果。可以看出，该模型仍有很大的作用，这是为什么？

其原因很难用上述两种思路解释，因为他们都认为预训练的数据和后续任务的数据是同分布或高度相关的。而这里显然程序语言和自然语言相差极大，为甚预训练依旧有效。一种可行的解释是套用迁移学习中关系迁移的概念，即样本之间的关系被迁移了而非样本本身被迁移。这种迁移被认为是无需i.i.d条件的。举例而言，自然语言中有“如果”，“假如”，“否则”等概念，而程序语言中有ifelse，他们的功能，或者说和其他元素的关系存在一定的共性。
https://www.aclweb.org/anthology/2020.findings-emnlp.139.pdf

![](https://i.imgur.com/L2rXtuk.png)


![](https://i.imgur.com/VB1l0UO.png)


## 预训练的出发点

### P(X|$\theta$)
预训练的一个出发点就是估计数据分布本身，LM，MLM，DAE都属于此类。
这类方法的本质都是去估计数据分布或者获得$P(X|\theta)$的一部分功能，比如DAE不能直接给出概率估计，但它可以模拟临近采样，同样的，MLM也可以通过Gibbs采样的方式获得文本，但其不能直接给出数据点的概率，以此类推，XLNet，word2vec等各式各样的预训练信号都具备$P(X|\theta)$的一部分功能。

### 目标任务相关的辅助任务
另一大类则是设计与目标任务有关的辅助任务

其具体又可以分为几大类型

#### 语义相关
比如利用同义词，复述等任务作为训练目标（比如借助WordNet）。这类任务的出发点是让模型理解语义，从而获得更强的语言理解能力

#### 结构相关
比如利用“因为、所以”，“虽然、但是”等信息，构建一个判断句子之间关系的预训练任务，输入两句话，根据上述关键词，判断其是因果关系，递进关系，并列关系或转折关系等。显然这一预训练任务对NLI是很有帮助的。以此类推，同样也可以利用Syntax信息进行预训练。

#### 上下文相关
还有一些预训练任务专注于让模型重视上下文信息，比如跳跃预测，即只给定较远的上下文信息而减少或去除近处的信息，迫使模型利用远处的上下文。

### 交互相关
此外，有些预训练方法强调交互性，比如机器翻译需要两句话交互，那预训练任务也会有相似的设计。

# 下期预告

## 预训练与迁移学习

## 为什么需要fine-tune
