讨论2021-05-11
=

# Pre-training 第二讲
## 预训练与迁移学习

迁移学习，顾名思义是把一个模型从一个地方迁移到另一个地方，而二者之间又存在着某种差异。对于监督学习来讲，一般可以做如下归类。假设模型输入为X，输出为Y，出发地为S，到达地为T，那么有五种差异，即$X_S$和$X_T$不同，即输入空间不同，比如中文和英文。$P(X_S)$和$P(X_T)$不同，即输入空间相同或高度重合，但分布不同，比如书面语言和口语。$Y_S$和$Y_T$不同，即输出空间不同，分好坏和分特性。$P(Y_S)$和$P(Y_T)$不同，即输出空间相同但分布不同，比如一边是好坏各半，另一边是90%好，10%坏。 $P(X_S, Y_S)$ $P(X_T, Y_T)$输入输出空间，边际概率都相同，但二者关系不同，比如有的人以胖为美，有的人以瘦为美，评价标准不同，但输入输出一样。迁移学习的目的就是寻找合适的迁移方法，让模型尽可能把学到的知识应用到目标问题。

假如我们的模型学习了如何分别以下四张图中的某一张，如何拓展到其他的三个？
| | |
|-|-|
|![](https://i.imgur.com/afnlti1.png )|![](https://i.imgur.com/jUv9nMZ.png )|
|![](https://i.imgur.com/m8mfzkS.jpg )|![](https://i.imgur.com/XIHNjDh.png )

### 传统迁移学习
传统迁移学习一般分四类，即基于实体（instance-based）, 基于特征（feature-based）, 基于参数（parameter-based）, 基于关系（relational-based）的迁移方法。我个人认为这四种方法可以通过数据和特征空间来解释，即我们面对两个空间，真实样本所在的数据空间和模型提取的特征所在的特征空间。想办法如何融合两地的数据空间，便是基于实体的方法，融合两地的特征空间，便是基于特征的方法。参数决定了数据到特征的映射函数，所以参数迁移实际是迁移了两地数据到特征空间的映射关系。关系则是多个实体或多个特征之间的联系，可以理解为迁移了两地的度量空间。

![](https://i.imgur.com/BfphUiK.png)

### 深度迁移学习
深度学习之后，由于模型的进步，训练数据的增大等多方面的因素，传统迁移学习的分类方法逐渐被淡化，转而出现一批深度迁移方法。主要分两类，一是领域适应，二是预训练。前者本质是拉近迁移双方的空间，可以是数据空间也可以是特征空间。具体方法可能是对抗学习，最小化分布间距离，比如MMD。而预训练方法又可以分为以下几类，fine-tune，增加额外模块后fine-tune，或直接使用。fine-tune即在目标训练集上二次训练，这一过程中可以调整全部参数，也可以保留一部分，增加一些额外的参数等。

此外，meta-learning同样属于一种预训练的方法。即如何让学习到的模型容易被迁移。

![](https://i.imgur.com/PzPKFOj.png)


### 如何保证迁移的有效性？

迁移学习中一个重要问题就是如何保证迁移是有效的，预训练也是同样的，为什么预训练可以在下游任务上取得效果，为什么是有帮助的？为此，我们需要考虑模型的哪些部分或能力是可以被迁移的，比如双方的输入空间是否一样，双方的输出空间是否有关，双方的联合概率是否有差异等。再根据差异对症下药，比如输入空间不同就可以用对抗学习，输出空间不同就可以引入额外参数，关系改变可以考虑只迁移部分参数。假如输入空间高度重合，但分布不同，这时可以采用重要性采样方法。

## 预告
### 预训练的局限和机遇

